{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasel\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'Linguistics/LinguisticsModel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = Word2Vec.load(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_scheme = {'genitives': '<b><font color=\"#A52A2A\">{}</font></b>'\n",
    "                , 'comparativ':'<span style=\"background-color: #66CDAA\">{}</span>'\n",
    "                , 'coordinate_NPs': '<span style=\"background-color: #228B22\">{}</span>'\n",
    "                , 'not in vocabulary': '<span style=\"background-color: #FFE4E1\">{}</span>'\n",
    "                , 'i vs we': '<b><font color=\"#8B008B\">{}</font></b>'\n",
    "                , 'imperative mood': '<span style=\"background-color: #DEB887\">{}</span>'\n",
    "                , 'subjunctive mood': '<span style=\"background-color: #BC8F8F\">{}</span>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLStyle:\n",
    "    def __init__(self, color_scheme=color_scheme):\n",
    "        self.color_scheme = color_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = HTMLStyle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs = []\n",
    "for k, v in color_scheme.items():\n",
    "    strs.append(style.color_scheme[k].format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цвет разных ошибок для примера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b><font color=\"#A52A2A\">genitives</font></b>   <span style=\"background-color: #66CDAA\">comparativ</span>   <span style=\"background-color: #228B22\">coordinate_NPs</span>   <span style=\"background-color: #FFE4E1\">not in vocabulary</span>   <b><font color=\"#8B008B\">i vs we</font></b>   <span style=\"background-color: #DEB887\">imperative mood</span>   <span style=\"background-color: #BC8F8F\">subjunctive mood</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('   '.join(strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from alphabet_detector import AlphabetDetector\n",
    "import json\n",
    "\n",
    "MORE_LESS = ['более', 'менее']\n",
    "STATISTICS = \"maxs\"\n",
    "MODEL = 'Linguistics/LinguisticsModel'\n",
    "\n",
    "class Searcher:\n",
    "    def __init__(self):\n",
    "        self.found = defaultdict(list)\n",
    "        self.flag_i_vs_we = ''\n",
    "        self.found_word = defaultdict(list)\n",
    "\n",
    "    def find_genitives(self, gen_chain, word, s, i, threshold=6):\n",
    "        if word['feats'] and 'Case' in word['feats'].keys() and word['feats']['Case'] == 'Gen':\n",
    "            gen_chain.append((word['form'], s, i))\n",
    "        else:\n",
    "            if len(gen_chain) >= int(threshold):\n",
    "                self.found['genitives'].append(gen_chain)\n",
    "                for gen in gen_chain:\n",
    "                    self.found_word[gen].append('genitives')\n",
    "            gen_chain = []\n",
    "        return gen_chain\n",
    "\n",
    "    def find_wrong_comparativ(self, sent, word, i, s):\n",
    "        if i+1 < len(sent):\n",
    "            next = sent[i + 1]\n",
    "            if word['form'] in MORE_LESS and 'comp' in next['feats']:\n",
    "                self.found['comparatives'].append((word['form'], next['form'], s, i))\n",
    "                self.found_word[(word['form'], s, i)].append('comparatives')\n",
    "\n",
    "    #!\n",
    "    def find_wrong_coordinate_NPs(self, sent, i, s, word, model):\n",
    "        if i+1 < len(sent):\n",
    "            if word['form'] == 'и':\n",
    "                t = i\n",
    "                pair = []\n",
    "                while (sent[t]['feats'] and 'S' not in sent[t]['feats']) and (sent[t]['feats'] and 'V' not in sent[t]['feats']) and t > 0:\n",
    "                    t -= 1\n",
    "                if sent[t]['feats'] and 'S' in sent[t]['feats']:\n",
    "                    pair.append(sent[t]['form'])\n",
    "                t = i\n",
    "                while (sent[t]['feats'] and 'S' not in sent[t]['feats']) and (sent[t]['feats'] and 'V' not in sent[t]['feats']) and t < len(sent):\n",
    "                    t += 1\n",
    "                if sent[t]['feats'] and 'S' in sent[t]['feats']:\n",
    "                    pair.append(sent[t]['form'])\n",
    "                if len(pair) > 1:\n",
    "                    if pair[0] in model.wv.vocab and pair[1] in model.wv.vocab:\n",
    "                        self.found['coordinate_NPs'].append(pair + [s, i, model.similarity(pair[0], pair[1])])\n",
    "                    else:\n",
    "                        self.found['coordinate_NPs'].append(pair + [s, i, float('-inf')])\n",
    "                    self.found_word[(pair[0], s, i)].append('coordinate_NPs')\n",
    "                    self.found_word[(pair[1], s, i+1)].append('coordinate_NPs')\n",
    "\n",
    "    def not_in_vocabulary(self, ad, word, i, model, s):\n",
    "        if word['form'].isalpha() and ad.only_alphabet_chars(word['form'], \"CYRILLIC\") and word[\n",
    "            'form'].lower() not in model.wv.vocab:\n",
    "            self.found['not in vocabulary'].append((word['form'], s, i))\n",
    "            self.found_word[(word['form'], s, i)].append('not in vocabulary')\n",
    "\n",
    "    def i_vs_we(self, i, word, s):\n",
    "        if word['lemma'] == 'Я' and not self.flag_i_vs_we:\n",
    "            self.flag_i_vs_we = 'i'\n",
    "            self.found['i vs we'].append((word['form'], s, i))\n",
    "            self.found_word[(word['form'], s, i)].append('i vs we')\n",
    "        elif (word['lemma'] == 'Я' and self.flag_i_vs_we == 'we') or (\n",
    "                word['lemma'] == 'МЫ' and self.flag_i_vs_we == 'i'):\n",
    "            self.found['i vs we'].append((word['form'], s, i))\n",
    "            self.found_word[(word['form'], s, i)].append('i vs we')\n",
    "        elif word['lemma'] == 'МЫ' and not self.flag_i_vs_we:\n",
    "            self.flag_i_vs_we = 'we'\n",
    "            self.found['i vs we'].append((word['form'], s, i))\n",
    "            self.found_word[(word['form'], s, i)].append('i vs we')\n",
    "\n",
    "    def check_mood(self, sent, i, word, s):\n",
    "        if word['form'] == 'бы' and i > 0:\n",
    "            self.found['subjunctive mood'].append((sent[i - 1]['form'], word['form'], s, i))\n",
    "            self.found_word[(sent[i - 1]['form'], s, i-1)].append('subjunctive mood')\n",
    "            self.found_word[(word['form'], s, i)].append('subjunctive mood')\n",
    "        if word['feats'] and 'Mood' in word['feats'].keys() and word['feats']['Mood'] == 'Imp':\n",
    "            self.found['imperative mood'].append((word['form'], s, i))\n",
    "            self.found_word[(word['form'], s, i)].append('imperative mood')\n",
    "\n",
    "\n",
    "    def check_all(self, tree):\n",
    "        #s - sentence number\n",
    "        #i - word number\n",
    "        logging.basicConfig(level=logging.INFO, filename='found.log')\n",
    "        model = Word2Vec.load(MODEL)\n",
    "        ad = AlphabetDetector()\n",
    "\n",
    "        for s, sent in enumerate(tqdm(tree)):\n",
    "            gen_chain = []\n",
    "\n",
    "            for i, word in enumerate(sent):\n",
    "                self.check_mood(sent, i, word, s)\n",
    "                self.i_vs_we(i, word, s)\n",
    "                self.not_in_vocabulary(ad, word, i, model, s)\n",
    "                gen_chain = self.find_genitives(gen_chain, word, s, i)\n",
    "                self.find_wrong_comparativ(sent, word, i, s)\n",
    "                self.find_wrong_coordinate_NPs(sent, i, s, word, model)\n",
    "\n",
    "        for key, value in self.found.items():\n",
    "            logging.info(key)\n",
    "            for mistake in value:\n",
    "                logging.info(mistake)\n",
    "                \n",
    "        return self.found_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ufal.udpipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse, parse_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline, ProcessingError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_model = Model.load('russian-syntagrus-ud-2.3-181115.udpipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Рассматривайте задачу, когда у нас есть куча обучающих примеров (изображений), но, к сожалению, их забыли разметить. \n",
    "Ну и есть небольшое подмножество примеров из той же выборки, которые все же напряглись и разметили по классам. И нам необходимо научиться решать задачу классификации. Правда, в размеченной выборке на каждый класс приходится маловато примеров: конкретно от 1 до 50, так что обычный supervised learning явно не взлетит. Авторы предлагают для решения метод, который они назвали CACTUs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(ud_model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pipeline.process(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = Searcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 265.90it/s]\n"
     ]
    }
   ],
   "source": [
    "check = searcher.check_all(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Найденные ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(('Рассматривайте', 0, 0), ['imperative mood']), (('когда', 0, 3), ['not in vocabulary']), (('у', 0, 4), ['not in vocabulary']), (('нас', 0, 5), ['not in vocabulary']), (('есть', 0, 6), ['not in vocabulary']), (('но', 0, 14), ['not in vocabulary']), (('к', 0, 16), ['not in vocabulary']), (('их', 0, 19), ['not in vocabulary']), (('разметить', 0, 21), ['not in vocabulary']), (('Ну', 1, 0), ['not in vocabulary']), (('и', 1, 1), ['not in vocabulary']), (('есть', 1, 2), ['not in vocabulary']), (('из', 1, 6), ['not in vocabulary']), (('же', 1, 8), ['not in vocabulary']), (('все', 1, 12), ['not in vocabulary']), (('же', 1, 13), ['not in vocabulary']), (('и', 1, 15), ['not in vocabulary']), (('разметили', 1, 16), ['not in vocabulary']), (('по', 1, 17), ['not in vocabulary']), (('И', 2, 0), ['not in vocabulary']), (('в', 3, 2), ['not in vocabulary']), (('размеченной', 3, 3), ['not in vocabulary']), (('на', 3, 5), ['not in vocabulary']), (('от', 3, 13), ['not in vocabulary']), (('до', 3, 15), ['not in vocabulary']), (('так', 3, 18), ['not in vocabulary']), (('что', 3, 19), ['not in vocabulary']), (('не', 3, 24), ['not in vocabulary']), (('взлетит', 3, 25), ['not in vocabulary']), (('для', 4, 2), ['not in vocabulary']), (('они', 4, 7), ['not in vocabulary'])])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    #readability_string = check_text(text)\n",
    "    out = pipeline.process(text)\n",
    "    tree = parse(out)\n",
    "    check = searcher.check_all(tree)\n",
    "\n",
    "    sents = []\n",
    "    for s, sent in enumerate(tree):\n",
    "        if sent[-1]['misc'] and sent[-1]['misc']['SpacesAfter'] and '\\\\n' in sent[-1]['misc']['SpacesAfter']:\n",
    "            s = [i for i in sent.metadata['text'].split()]\n",
    "            s.append('\\n')\n",
    "            sents.append(s)\n",
    "        else:\n",
    "            sents.append([i for i in sent.metadata['text'].split()])\n",
    "\n",
    "    out_t = []\n",
    "    for i in range(len(sents)):\n",
    "        for j in range(len(sents[i])):\n",
    "            if (sents[i][j], i, j) in check.keys():\n",
    "                word = str(sents[i][j])\n",
    "                out_t.append(style.color_scheme[check[(word, i, j)][0]].format(word))\n",
    "            else:\n",
    "                out_t.append(sents[i][j])\n",
    "\n",
    "    string = ' '.join(out_t)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 3338.88it/s]\n"
     ]
    }
   ],
   "source": [
    "string = process_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<span style=\"background-color: #DEB887\">Рассматривайте</span> задачу, когда у нас есть куча обучающих примеров (изображений), но, к сожалению, их забыли разметить. \\n <span style=\"background-color: #FFE4E1\">Ну</span> <span style=\"background-color: #FFE4E1\">и</span> <span style=\"background-color: #FFE4E1\">есть</span> небольшое подмножество примеров <span style=\"background-color: #FFE4E1\">из</span> той <span style=\"background-color: #FFE4E1\">же</span> выборки, которые все же напряглись и разметили по классам. <span style=\"background-color: #FFE4E1\">И</span> нам необходимо научиться решать задачу классификации. Правда, в размеченной выборке на каждый класс приходится маловато примеров: конкретно от 1 до 50, так что обычный supervised learning явно не взлетит. Авторы предлагают <span style=\"background-color: #FFE4E1\">для</span> решения метод, который они назвали CACTUs \\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #DEB887\">Рассматривайте</span> задачу, когда у нас есть куча обучающих примеров (изображений), но, к сожалению, их забыли разметить. \n",
       " <span style=\"background-color: #FFE4E1\">Ну</span> <span style=\"background-color: #FFE4E1\">и</span> <span style=\"background-color: #FFE4E1\">есть</span> небольшое подмножество примеров <span style=\"background-color: #FFE4E1\">из</span> той <span style=\"background-color: #FFE4E1\">же</span> выборки, которые все же напряглись и разметили по классам. <span style=\"background-color: #FFE4E1\">И</span> нам необходимо научиться решать задачу классификации. Правда, в размеченной выборке на каждый класс приходится маловато примеров: конкретно от 1 до 50, так что обычный supervised learning явно не взлетит. Авторы предлагают <span style=\"background-color: #FFE4E1\">для</span> решения метод, который они назвали CACTUs \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
